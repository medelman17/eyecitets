---
phase: 02-core-parsing
plan: 03
type: execute
wave: 2
depends_on: [02-01, 02-02]
files_modified:
  - src/tokenize/tokenizer.ts
  - src/tokenize/index.ts
  - tests/tokenize/tokenizer.test.ts
autonomous: true

must_haves:
  truths:
    - "Developer can tokenize cleaned text into citation candidates with positions"
    - "Tokenizer applies all patterns from Plan 2 with timeout protection"
    - "Tokens include matched text, span, and pattern metadata"
  artifacts:
    - path: "src/tokenize/tokenizer.ts"
      provides: "tokenize() function applying patterns to text"
      exports: ["tokenize", "Token"]
      min_lines: 60
    - path: "tests/tokenize/tokenizer.test.ts"
      provides: "Tokenization validation"
      min_lines: 40
  key_links:
    - from: "src/tokenize/tokenizer.ts"
      to: "src/patterns/index.ts"
      via: "Pattern import for tokenization"
      pattern: "import.*Patterns.*from.*patterns"
    - from: "src/tokenize/tokenizer.ts"
      to: "src/types/span.ts"
      via: "Span type for token positions"
      pattern: "import.*Span.*from.*span"
---

<objective>
Implement tokenization layer that applies regex patterns to cleaned text and produces citation candidate tokens. Tokens include matched text, clean span positions, pattern type, and metadata for extraction layer (Plan 5).

Purpose: Converts cleaned text into structured tokens for citation extraction
Output: tokenize() function, Token interface, tokenization tests
</objective>

<execution_context>
@/Users/medelman/.claude/get-shit-done/workflows/execute-plan.md
@/Users/medelman/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-core-parsing/02-CONTEXT.md
@.planning/phases/02-core-parsing/02-RESEARCH.md
@.planning/phases/02-core-parsing/02-01-SUMMARY.md
@.planning/phases/02-core-parsing/02-02-SUMMARY.md
@src/types/span.ts
@src/patterns/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Define Token interface and tokenize() function</name>
  <files>src/tokenize/tokenizer.ts, src/tokenize/index.ts</files>
  <action>
Create `src/tokenize/tokenizer.ts` with:

```typescript
import type { Span } from '@/types/span'
import type { Pattern } from '@/patterns'

export interface Token {
  /** Matched text from input */
  text: string

  /** Position in cleaned text (cleanStart/cleanEnd only, no original positions yet) */
  span: Pick<Span, 'cleanStart' | 'cleanEnd'>

  /** Pattern type that matched this token */
  type: Pattern['type']

  /** Pattern ID that matched this token */
  patternId: string
}

export function tokenize(
  cleanedText: string,
  patterns: Pattern[]
): Token[]
```

Implementation:
1. For each pattern in patterns array:
   - Apply pattern.regex.matchAll(cleanedText)
   - For each match, create Token with { text: match[0], span: { cleanStart: match.index, cleanEnd: match.index + match[0].length }, type: pattern.type, patternId: pattern.id }
2. Collect all tokens from all patterns
3. Sort tokens by cleanStart position (ascending)
4. Return sorted token array

**Timeout protection:** Wrap matchAll() in try-catch. If pattern throws, skip pattern and log warning to console. Continue with remaining patterns.

**Note:** tokenize() is synchronous because regex matching is inherently synchronous. This enables both sync (extractCitations) and async (extractCitationsAsync) APIs in Plan 6.

Import all pattern arrays (casePatterns, statutePatterns, etc.) and concatenate for default patterns parameter.

Create `src/tokenize/index.ts` with re-exports: `export * from "./tokenizer"`.

Add JSDoc explaining tokenization purpose (broad matching, not metadata extraction).
  </action>
  <verify>
Run `npm run typecheck` - no errors
Check that tokenize() is exported from src/tokenize/index.ts
Verify Token interface has all required fields
  </verify>
  <done>tokenize() function exists, applies patterns to text, returns sorted tokens with positions</done>
</task>

<task type="auto">
  <name>Task 2: Test tokenization on sample legal text</name>
  <files>tests/tokenize/tokenizer.test.ts</files>
  <action>
Create `tests/tokenize/tokenizer.test.ts` with Vitest tests:

**Test 1: Tokenize case citation**
- Input: "Smith v. Doe, 500 F.2d 123 (9th Cir. 2020)"
- Expected: Token with text "500 F.2d 123", type "case", patternId "federal-reporter"
- Assert: Token span positions match expected (cleanStart/cleanEnd point to "500 F.2d 123")

**Test 2: Tokenize statute citation**
- Input: "See 42 U.S.C. ยง 1983 for details"
- Expected: Token with text matching "42 U.S.C. ยง 1983", type "statute"
- Assert: Token found at correct position

**Test 3: Tokenize multiple citations in text**
- Input: "In Smith v. Doe, 500 F.2d 123, the court cited 42 U.S.C. ยง 1983"
- Expected: Two tokens (case + statute), sorted by position
- Assert: First token is case, second is statute

**Test 4: Tokenize text with no citations**
- Input: "This is plain text with no legal citations"
- Expected: Empty array
- Assert: tokens.length === 0

**Test 5: Tokenize overlapping patterns** (edge case)
- Input: "123 U.S. 456" (matches both supreme-court pattern and potentially state-reporter pattern)
- Expected: At least one token with type "case"
- Assert: Token found (exact matching details determined by pattern precedence)

Use `tokenize(input, allPatterns)` syntax (sync). Import all pattern arrays for testing.

Add descriptive test names and comments.
  </action>
  <verify>
Run `npm test tests/tokenize/tokenizer.test.ts` - all tests pass
Test coverage for src/tokenize/tokenizer.ts >= 80%
Verify tokens have correct span positions
  </verify>
  <done>Tokenization tests pass, validating pattern application and token structure</done>
</task>

</tasks>

<verification>
1. Run `npm run typecheck` - no TypeScript errors
2. Run `npm test tests/tokenize/` - all tokenization tests pass
3. Check that tokenize() correctly applies patterns from Plan 2
4. Verify tokens are sorted by position (cleanStart ascending)
5. Test coverage >= 80% for src/tokenize/ directory
</verification>

<success_criteria>
- Developer can tokenize cleaned text into citation candidate tokens
- Tokens include matched text, clean span, pattern type, and pattern ID
- Tokenizer applies all patterns from Plan 2 with timeout protection
- Tokenization tests validate correct token structure and positions
- All code compiles with TypeScript strict mode
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-parsing/02-03-SUMMARY.md`
</output>
